<!DOCTYPE html><html lang="en"><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content="Personal website of Aria Wang."><title>Aria Wang • 王 元</title><link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous"><style> body.dark-theme{background-color:#1c1c1e;color:#bcbcba}body.dark-theme a:link{color:CornflowerBlue}body.dark-theme a:visited{color:DeepSkyBlue}body.dark-theme a:hover{color:DodgerBlue}body.dark-theme figure img{filter:invert(90%);border:1px solid black}a:link{color:MediumBlue}a:visited{color:DarkBlue}a:hover{color:Blue}p{text-align:justify;text-justify:inter-word}.container{padding-top:2rem;padding-bottom:2rem;overflow-x:hidden}.math-display-wrap{display:block;overflow-x:auto;white-space:nowrap}.math-display-wrap .katex-display{margin:0.5em 0}svg{stroke:currentColor;stroke-linecap:round;stroke-linejoin:round}#title{margin-bottom:1rem}#theme-switch{margin-left:2rem}#theme-switch svg{width:24px;height:24px}#theme-switch:hover{cursor:pointer}abbr,.abbr{font-size:smaller;font-variant:small-caps;letter-spacing:0.1em;text-decoration:none !important}table{width:100%;margin-top:2rem;margin-bottom:2rem;border-collapse:collapse}table td,table th{padding:.75rem;vertical-align:top}table th{border-top:1px solid;border-bottom:1px solid}figure{text-align:center;margin-top:2rem;margin-bottom:2rem}figure img{max-width:100%;height:auto}figure figcaption{margin-top:1rem}.subfigures p{text-align:center}.sup-left{padding-right:2px}.sup-right{padding-left:2px}h1 .header-section-number,h2 .header-section-number,h3 .header-section-number{font-size:smaller;font-weight:normal}h1 .header-section-number:after,h2 .header-section-number:after,h3 .header-section-number:after{content:"."}h4 .header-section-number,h5 .header-section-number,h6 .header-section-number{display:none}.citation a{text-decoration:none}#refs{font-size:smaller}#refs div{margin:1rem 0}.footnotes{font-size:smaller} a.icon-link{color:inherit !important;text-decoration:none}a.icon-link svg{fill:none;width:40px;height:24px}</style><div class="container"><div class="row justify-content-center"><div class="col-12 col-sm-12 col-md-11 col-lg-9 col-xl-8"><h1 id="title"> <small aria-disabled="true" id="theme-switch" class="float-end"></small>Aria Wang • 王 元</h1><p><a title="Email" class="icon-link" href="mailto:ariawang@cmu.edu"><svg><path stroke="none" d="M0 0h24v24H0z"/> <rect x="3" y="5" width="18" height="14" rx="2" /><polyline points="3 7 12 13 21 7" /> </svg></a><a title="GitHub" class="icon-link" href="https://www.github.com/ariaaay"><svg><path stroke="none" d="M0 0h24v24H0z"/><path d="M9 19c-4.286 1.35-4.286-2.55-6-3m12 5v-3.5c0-1 .099-1.405-.5-2 2.791-.3 5.5-1.366 5.5-6.04a4.567 4.567 0 0 0 -1.333 -3.21 4.192 4.192 0 00-.08-3.227s-1.05-.3-3.476 1.267a12.334 12.334 0 0 0 -6.222 0C6.462 2.723 5.413 3.023 5.413 3.023a4.192 4.192 0 0 0 -.08 3.227A4.566 4.566 0 004 9.486c0 4.64 2.709 5.68 5.5 6.014-.591.589-.56 1.183-.5 2V21" /> </svg></a><a title="Google Scholar" class="icon-link" href="https://scholar.google.com/citations?user=MepHbYgAAAAJ&hl=en"><svg><path stroke="none" d="M0 0h24v24H0z"/><path d="M17.788 5.108A9 9 0 1021 12h-8" /> </svg></a><a title="Twitter" class="icon-link" href="https://twitter.com/ariairaw"><svg><path stroke="none" d="M0 0h24v24H0z"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84 0 0 0 .497 -3.753C20.18 7.773 21.692 5.25 22 4.009z" /> </svg></a><p>I am a joint PhD student in <a href="http://compneuro.cmu.edu/">Neural Computation</a> and <a href="https://www.ml.cmu.edu/">Machine Learning</a> at Carnegie Mellon University. I am advised by <a href="https://www.cmu.edu/dietrich/psychology/people/core-training-faculty/tarr-michael.html">Mike Tarr</a> and <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a>.<p>I study visual and semantic processing in the human brain. In my work, I collect data using functional Magnetic Resonance Imaging (fMRI) and use models from computer vision and natural language processing to model brain responses of viewing natural images and movies.<p>Before joining CMU, I received a B.A. in Cognitive Science and Statistics from UC Berkeley.<p>Other than research, I enjoy rock climbing, mountaineering, cooking, and reading.<h1 class="unnumbered" id="publications-and-preprints">Publications and Preprints</h1><p style="text-align: left"> <strong>Learning Intermediate Features of Object Affordances with a Convolutional Neural Network</strong> [<a href="https://arxiv.org/pdf/2002.08975.pdf">paper</a>]<br><u>Aria Y. Wang</u>, Michael J. Tarr<br><i>Conference on Cognitive Computational Neuroscience (CCN) 2018</i><br><p style="text-align: left"> <strong>Neural Taskonomy: Inferring the Similarity of Task-Derived Representations from Brain Activity</strong> [<a href="http://www.cs.cmu.edu/~neural-taskonomy/src/paper.pdf">paper</a>] [<a href="pdf/TaskonomyPoster.pdf">poster</a>] [<a href="http://www.cs.cmu.edu/~neural-taskonomy/">website</a>]<br><u>Aria Y. Wang</u>, Michael J. Tarr, Leila Wehbe<br><i>Neural Information Processing Systems (NeurIPS) 2019</i><br><p style="text-align: left"> <strong>Joint Interpretation of Representations in Neural Network and the Brain</strong> [<a href="pdf/JointInterpretationPaper.pdf">paper</a>]<br><u>Aria Y. Wang</u><sup class="sup-right">*</sup>, Ruogu Lin<sup class="sup-right">*</sup>, Michael J. Tarr, Leila Wehbe<br><i>ICLR Workshop 2021</i><br><p style="text-align: left"> <strong>Food for Thought: Selectivity for Food in Human Ventral Visual Cortex</strong> [<a href="https://www.biorxiv.org/content/10.1101/2022.05.22.492983v2.full.pdf">paper</a>]<br>Nidhi Jain, <u>Aria Y. Wang</u>, Margaret M Henderson, Ruogu Lin, Jacob S Prince, Michael J. Tarr, Leila Wehbe<br><i>In Review.</i><br><p style="text-align: left"> <strong>Incorporating Natural Language into Vision Models Improves Prediction and Understanding of Higher Visual Cortex</strong> [<a href="https://www.biorxiv.org/content/biorxiv/early/2022/09/29/2022.09.27.508760.full.pdf">paper</a>] [<a href="pdf/CLIPPoster.pdf">poster</a>] [<a href="https://www.youtube.com/watch?v=CnFX0EzKbXU">talk</a>]<br><u>Aria Y. Wang</u>, Kendrick Kay, Thomas Naselaris, Michael J. Tarr, Leila Wehbe<br><i>In Review.</i><br><p class="small"> <sup class="sup-left">*</sup>Equal contribution<h1 class="unnumbered" id="conference-and-workshop-presentations">Conference and Workshop Presentations</h1><p style="text-align: left"> <strong>Learning Intermediate Features of Affordances with a Convolutional Neural Network</strong><br>Aria Y. Wang, Michael J. Tarr<br><i>Poster Presented at Annual Meeting of the Vision Science Society (VSS) 2018</i><br><p style="text-align: left"> <strong>Learning Intermediate Features of Affordances with a Convolutional Neural Network</strong><br>Aria Y. Wang, Michael J. Tarr<br><i>Poster presented at Conference on Cognitive Computational Neuroscience (CCN) 2018</i><br><p style="text-align: left"> <strong>Neural Taskonomy: Inferring the Similarity of Task-Derived Representations from Brain Activity</strong><br>Aria Y. Wang, Leila Wehbe, Michael J. Tarr<br><i>Poster presented at Algonauts Workshop at MIT</i><br><p style="text-align: left"> <strong>Expanding Visual Feature Spaces towards a General Encoding Model of Scene Perception</strong><br>Aria Y. Wang, Michael J. Tarr, Leila Wehbe<br><i>Poster presented at Society for Neuroscience (SfN) 2019</i><br><p class="small"> <sup class="sup-left">*</sup>Equal contribution</div></div></div><script src="https://code.jquery.com/jquery-3.6.0.slim.min.js" integrity="sha256-u7e5khyithlIdTpu22PHhENmPcRdFiHRjhAuHcs05RI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js" integrity="sha384-QJHtvGhmr9XOIpI6YVutG+2QOK9T+ZnN4kzFN1RtK3zEFEIsxhlmWl5/YESvpZ13" crossorigin="anonymous"></script> <script> function setTheme(e,t=!0){"dark"===e?$("body").addClass("dark-theme"):$("body").removeClass("dark-theme"),"dark"===("dark"===e?"light":"dark")?$("#theme-switch").html('<svg style="fill: currentColor"><path d="M16.2 4a9.03 9.03 0 1 0 3.9 12a6.5 6.5 0 1 1 -3.9 -12" /></svg>'):$("#theme-switch").html('<svg style="fill: none"><circle cx="12" cy="12" r="4" /><path d="M3 12h1M12 3v1M20 12h1M12 20v1M5.6 5.6l.7 .7M18.4 5.6l-.7 .7M17.7 17.7l.7 .7M6.3 17.7l-.7 .7" /></svg>'),!0===t&&($(".local-link").each(function(){var t=$(this)[0].href.split("?",1)[0];$(this)[0].href=t+"?"+e}),history.replaceState({theme:!0},document.title,"?"+e))}function getURLParameter(e){for(var t=window.location.search.substring(1).split("&"),a=0;a<t.length;a++){var r=t[a].split("=");if(r[0]===e)return r[1]===undefined||decodeURIComponent(r[1])}}$("#theme-switch").on("click",function(){setTheme($("body").hasClass("dark-theme")?"light":"dark")}),$(document).ready(function(){var e=getURLParameter("dark"),t=getURLParameter("light");!0===e?setTheme("dark"):!0===t?setTheme("light"):window.matchMedia("(prefers-color-scheme: dark)").matches?setTheme("dark",modLinks=!1):setTheme("light",modLinks=!1)}); </script>
