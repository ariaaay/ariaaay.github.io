<!DOCTYPE html><html lang="en"><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content="Personal website of Aria Wang."><title>Aria Wang • 王 元</title><link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous"><style> body.dark-theme{background-color:#1c1c1e;color:#bcbcba}body.dark-theme a:link{color:CornflowerBlue}body.dark-theme a:visited{color:DeepSkyBlue}body.dark-theme a:hover{color:DodgerBlue}body.dark-theme figure img{filter:invert(90%);border:1px solid black}a:link{color:MediumBlue}a:visited{color:DarkBlue}a:hover{color:Blue}p{text-align:justify;text-justify:inter-word}.container{padding-top:2rem;padding-bottom:2rem;overflow-x:hidden}.math-display-wrap{display:block;overflow-x:auto;white-space:nowrap}.math-display-wrap .katex-display{margin:0.5em 0}svg{stroke:currentColor;stroke-linecap:round;stroke-linejoin:round}#title{margin-bottom:1rem}#theme-switch{margin-left:2rem}#theme-switch svg{width:24px;height:24px}#theme-switch:hover{cursor:pointer}abbr,.abbr{font-size:smaller;font-variant:small-caps;letter-spacing:0.1em;text-decoration:none !important}table{width:100%;margin-top:2rem;margin-bottom:2rem;border-collapse:collapse}table td,table th{padding:.75rem;vertical-align:top}table th{border-top:1px solid;border-bottom:1px solid}figure{text-align:center;margin-top:2rem;margin-bottom:2rem}figure img{max-width:100%;height:auto}figure figcaption{margin-top:1rem}.subfigures p{text-align:center}.sup-left{padding-right:2px}.sup-right{padding-left:2px}h1 .header-section-number,h2 .header-section-number,h3 .header-section-number{font-size:smaller;font-weight:normal}h1 .header-section-number:after,h2 .header-section-number:after,h3 .header-section-number:after{content:"."}h4 .header-section-number,h5 .header-section-number,h6 .header-section-number{display:none}.citation a{text-decoration:none}#refs{font-size:smaller}#refs div{margin:1rem 0}.footnotes{font-size:smaller} a.icon-link{color:inherit !important;text-decoration:none}a.icon-link svg{fill:none;width:40px;height:24px}</style><div class="container"><div class="row justify-content-center"><div class="col-12 col-sm-12 col-md-11 col-lg-9 col-xl-8"><h1 id="title"> <small aria-disabled="true" id="theme-switch" class="float-end"></small>Aria Wang • 王 元</h1><p>[<a href="mailto:ariawang@cmu.edu">email</a>] [<a href="https://www.github.com/ariaaay">github</a>] [<a href="https://scholar.google.com/citations?user=MepHbYgAAAAJ&hl=en">google scholar</a>] [<a href="https://twitter.com/ariairaw">twitter</a>]<p>I am a joint PhD student in <a href="http://compneuro.cmu.edu/">Neural Computation</a> and <a href="https://www.ml.cmu.edu/">Machine Learning</a> at Carnegie Mellon University. I am advised by <a href="https://www.cmu.edu/dietrich/psychology/people/core-training-faculty/tarr-michael.html">Mike Tarr</a> and <a href="https://www.cs.cmu.edu/~lwehbe/">Leila Wehbe</a>.<p>I study visual and semantic processing in the human brain. In my work, I collect data using functional Magnetic Resonance Imaging (fMRI) and use models from computer vision and natural language processing to model brain responses of viewing natural images and movies.<p>Before joining CMU, I received a B.A. in Cognitive Science and Statistics from UC Berkeley.<p>Other than research, I enjoy rock climbing, mountaineering, cooking, and reading.<h1 class="unnumbered" id="publications-and-preprints">Publications and Preprints</h1><p><p><strong>Incorporating Natural Language into Vision Models Improves Prediction and Understanding of Higher Visual Cortex</strong><p>[<a href="https://www.biorxiv.org/content/biorxiv/early/2022/09/29/2022.09.27.508760.full.pdf">paper</a>]<p>[<a href="pdf/CLIPPoster.pdf">poster</a>]<p>[<a href="https://www.youtube.com/watch?v=CnFX0EzKbXU">talk</a>]<p><br><p><u>Aria Y. Wang</u>,<p>Kendrick Kay,<p>Thomas Naselaris,<p>Michael J. Tarr, Leila Wehbe <br> <i>In Review.</i><br><p><p><strong>Food for Thought: Selectivity for Food in Human Ventral Visual Cortex</strong><p>[<a href="https://www.biorxiv.org/content/10.1101/2022.05.22.492983v2.full.pdf">paper</a>]<p><br><p>Nidhi Jain,<p><u>Aria Y. Wang</u>,<p>Margaret M Henderson,<p>Ruogu Lin,<p>Jacob S Prince,<p>Michael J. Tarr, Leila Wehbe <br> <i>In Review.</i><br><p><p><strong>Joint Interpretation of Representations in Neural Network and the Brain</strong><p>[<a href="pdf/JointInterpretationPaper.pdf">paper</a>]<p><br><p><u>Aria Y. Wang</u><sup>*</sup>,<p>Ruogu Lin<sup>*</sup>,<p>Michael J. Tarr, Leila Wehbe <br> <i>ICLR Workshop 2021</i><br><p><p><strong>Neural Taskonomy: Inferring the Similarity of Task-Derived Representations from Brain Activity</strong><p>[<a href="http://www.cs.cmu.edu/~neural-taskonomy/src/paper.pdf">paper</a>]<p>[<a href="pdf/TaskonomyPoster.pdf">poster</a>]<p>[<a href="http://www.cs.cmu.edu/~neural-taskonomy/">website</a>]<p><br><p><u>Aria Y. Wang</u>,<p>Michael J. Tarr, Leila Wehbe <br> <i>Neural Information Processing Systems (NeurIPS) 2019</i><br><p><p><strong>Learning Intermediate Features of Object Affordances with a Convolutional Neural Network</strong><p>[<a href="https://arxiv.org/pdf/2002.08975.pdf">paper</a>]<p><br><p><u>Aria Y. Wang</u>, Michael J. Tarr <br> <i>Conference on Cognitive Computational Neuroscience (CCN) 2018</i><br><h1 class="unnumbered" id="conference-and-workshop-presentations">Conference and Workshop Presentations</h1><p><p><strong>Expanding Visual Feature Spaces towards a General Encoding Model of Scene Perception</strong> <br><p>Aria Y. Wang,<p>Michael J. Tarr, Leila Wehbe <br> <i>Poster presented at Society for Neuroscience (SfN) 2019</i><br><p><p><strong>Neural Taskonomy: Inferring the Similarity of Task-Derived Representations from Brain Activity</strong> <br><p>Aria Y. Wang,<p>Leila Wehbe, Michael J. Tarr <br> <i>Poster presented at Algonauts Workshop at MIT</i><br><p><p><strong>Learning Intermediate Features of Affordances with a Convolutional Neural Network</strong> <br><p>Aria Y. Wang, Michael J. Tarr <br> <i>Poster presented at Conference on Cognitive Computational Neuroscience (CCN) 2018</i><br><p><p><strong>Learning Intermediate Features of Affordances with a Convolutional Neural Network</strong> <br><p>Aria Y. Wang, Michael J. Tarr <br> <i>Poster Presented at Annual Meeting of the Vision Science Society (VSS) 2018</i><br></div></div></div><script src="https://code.jquery.com/jquery-3.6.0.slim.min.js" integrity="sha256-u7e5khyithlIdTpu22PHhENmPcRdFiHRjhAuHcs05RI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js" integrity="sha384-QJHtvGhmr9XOIpI6YVutG+2QOK9T+ZnN4kzFN1RtK3zEFEIsxhlmWl5/YESvpZ13" crossorigin="anonymous"></script> <script> function setTheme(e,t=!0){"dark"===e?$("body").addClass("dark-theme"):$("body").removeClass("dark-theme"),"dark"===("dark"===e?"light":"dark")?$("#theme-switch").html('<svg style="fill: currentColor"><path d="M16.2 4a9.03 9.03 0 1 0 3.9 12a6.5 6.5 0 1 1 -3.9 -12" /></svg>'):$("#theme-switch").html('<svg style="fill: none"><circle cx="12" cy="12" r="4" /><path d="M3 12h1M12 3v1M20 12h1M12 20v1M5.6 5.6l.7 .7M18.4 5.6l-.7 .7M17.7 17.7l.7 .7M6.3 17.7l-.7 .7" /></svg>'),!0===t&&($(".local-link").each(function(){var t=$(this)[0].href.split("?",1)[0];$(this)[0].href=t+"?"+e}),history.replaceState({theme:!0},document.title,"?"+e))}function getURLParameter(e){for(var t=window.location.search.substring(1).split("&"),a=0;a<t.length;a++){var r=t[a].split("=");if(r[0]===e)return r[1]===undefined||decodeURIComponent(r[1])}}$("#theme-switch").on("click",function(){setTheme($("body").hasClass("dark-theme")?"light":"dark")}),$(document).ready(function(){var e=getURLParameter("dark"),t=getURLParameter("light");!0===e?setTheme("dark"):!0===t?setTheme("light"):window.matchMedia("(prefers-color-scheme: dark)").matches?setTheme("dark",modLinks=!1):setTheme("light",modLinks=!1)}); </script>
